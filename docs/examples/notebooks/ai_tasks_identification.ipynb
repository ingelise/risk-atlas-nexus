{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "#### This notebook illustrates how to identify AI tasks based on specific use cases.\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Import libraries\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "/Users/ingevejs/Documents/workspace/ingelise/risk-atlas-nexus/src/ai_atlas_nexus/toolkit/job_utils.py:4: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                  "  from tqdm.autonotebook import tqdm\n"
               ]
            }
         ],
         "source": [
            "from ai_atlas_nexus.blocks.inference import (\n",
            "    RITSInferenceEngine,\n",
            "    WMLInferenceEngine,\n",
            "    OllamaInferenceEngine,\n",
            "    VLLMInferenceEngine,\n",
            ")\n",
            "from ai_atlas_nexus.blocks.inference.params import (\n",
            "    InferenceEngineCredentials,\n",
            "    RITSInferenceEngineParams,\n",
            "    WMLInferenceEngineParams,\n",
            "    OllamaInferenceEngineParams,\n",
            "    VLLMInferenceEngineParams,\n",
            ")\n",
            "from ai_atlas_nexus.library import AIAtlasNexus"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### AI Atlas Nexus uses Large Language Models (LLMs) to infer risks dimensions. Therefore requires access to LLMs to inference or call the model.\n",
            "\n",
            "**Available Inference Engines**: WML, Ollama, vLLM, RITS. Please follow the [Inference APIs](https://github.com/IBM/ai-atlas-nexus?tab=readme-ov-file#install-for-inference-apis) guide before going ahead.\n",
            "\n",
            "_Note:_ RITS is intended solely for internal IBM use and requires TUNNELALL VPN for access.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[2025-11-27 14:49:18:12] - INFO - AIAtlasNexus - OLLAMA inference engine will execute requests on the server at http://localhost:11434.\n",
                  "[2025-11-27 14:49:18:43] - INFO - AIAtlasNexus - Created OLLAMA inference engine.\n"
               ]
            }
         ],
         "source": [
            "inference_engine = OllamaInferenceEngine(\n",
            "    model_name_or_path=\"llama3:latest\",\n",
            "    credentials=InferenceEngineCredentials(api_url=\"OLLAMA_API_URL\"),\n",
            "    parameters=OllamaInferenceEngineParams(\n",
            "        num_predict=1000, temperature=0, repeat_penalty=1, num_ctx=8192\n",
            "    ),\n",
            ")\n",
            "\n",
            "# inference_engine = WMLInferenceEngine(\n",
            "#     model_name_or_path=\"ibm/granite-20b-code-instruct\",\n",
            "#     credentials={\n",
            "#         \"api_key\": \"WML_API_KEY\",\n",
            "#         \"api_url\": \"WML_API_URL\",\n",
            "#         \"project_id\": \"WML_PROJECT_ID\",\n",
            "#     },\n",
            "#     parameters=WMLInferenceEngineParams(\n",
            "#         max_new_tokens=1000, decoding_method=\"greedy\", repetition_penalty=1\n",
            "#     ),\n",
            "# )\n",
            "\n",
            "# inference_engine = VLLMInferenceEngine(\n",
            "#     model_name_or_path=\"ibm-granite/granite-3.1-8b-instruct\",\n",
            "#     credentials=InferenceEngineCredentials(\n",
            "#         api_url=\"VLLM_API_URL\", api_key=\"VLLM_API_KEY\"\n",
            "#     ),\n",
            "#     parameters=VLLMInferenceEngineParams(max_tokens=1000, temperature=0.7),\n",
            "# )\n",
            "\n",
            "# inference_engine = RITSInferenceEngine(\n",
            "#     model_name_or_path=\"ibm-granite/granite-3.1-8b-instruct\",\n",
            "#     credentials={\n",
            "#         \"api_key\": \"RITS_API_KEY\",\n",
            "#         \"api_url\": \"RITS_API_URL\",\n",
            "#     },\n",
            "#     parameters=RITSInferenceEngineParams(max_tokens=1000, temperature=0.7),\n",
            "# )"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### Create an instance of AIAtlasNexus\n",
            "\n",
            "_Note: (Optional)_ You can specify your own directory in `AIAtlasNexus(base_dir=<PATH>)` to utilize custom AI ontologies. If left blank, the system will use the provided AI ontologies.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "[2025-11-27 14:49:22:47] - INFO - AIAtlasNexus - Created AIAtlasNexus instance. Base_dir: None\n"
               ]
            }
         ],
         "source": [
            "ai_atlas_nexus = AIAtlasNexus()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "##### AI Tasks Identification API\n",
            "\n",
            "AIAtlasNexus.identify_ai_tasks_from_usecases()\n",
            "\n",
            "Params:\n",
            "\n",
            "- usecases (List[str]): A List of strings describing AI usecases\n",
            "- inference_engine (InferenceEngine): An LLM inference engine to identify AI tasks from usecases.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Inferring with OLLAMA: 100%|██████████| 1/1 [00:08<00:00,  8.04s/it]\n"
               ]
            },
            {
               "data": {
                  "text/plain": [
                     "['Text Classification',\n",
                     " 'Summarization',\n",
                     " 'Question Answering',\n",
                     " 'Text Generation',\n",
                     " 'Translation']"
                  ]
               },
               "execution_count": 5,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "usecase = \"Generate personalized, relevant responses, recommendations, and summaries of claims for customers to support agents to enhance their interactions with customers.\"\n",
            "\n",
            "risks = ai_atlas_nexus.identify_ai_tasks_from_usecases(\n",
            "    usecases=[usecase],\n",
            "    inference_engine=inference_engine,\n",
            ")\n",
            "\n",
            "risks[0].prediction"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "vrisk-atlas-nexus",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.12.4"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
