id: https://ibm.github.io/ai-atlas-nexus/ontology/ai_eval
name: ai_eval
description: Defines vocabulary relating to AI model evaluation
imports:
  - linkml:types
  - common
  - ai_risk
default_curi_maps:
  - semweb_context
prefixes:
  linkml: https://w3id.org/linkml/
  nexus: https://ibm.github.io/ai-atlas-nexus/ontology/
  dqv: https://www.w3.org/TR/vocab-dqv/
  skos: http://www.w3.org/2004/02/skos/core/
default_range: string
default_prefix: nexus

classes:
  AiEval:
    is_a: Entity
    class_uri: dqv:Metric
    description: An AI Evaluation, e.g. a metric, benchmark, unitxt card evaluation, a question or a combination of such entities.
    slot_usage:
      isComposedOf:
        range: AiEval
        description: A relationship indicating that an AI evaluation maybe composed of other AI evaluations (e.g. it's an overall average of other scores).
    slots:
      - hasDocumentation
      - hasDataset
      - hasTasks
      - hasImplementation
      - hasUnitxtCard
      - hasLicense
      - hasRelatedRisk
      - bestValue
      - hasBenchmarkMetadata

  AiEvalResult:
    is_a: Entity
    class_uri: dqv:QualityMeasurement
    description: The result of an evaluation for a specific AI model.
    mixins:
      - Fact
    slots:
      - isResultOf

  SourceMetadata:
    is_a: Entity
    class_uri: nexus:sourcemetadata
    description: Metadata about the source of an evaluation
    attributes:
      source_name:
        description: Name of the evaluation source
        range: string
      source_type:
        description: Type of source (e.g., evaluation_run)
        range: string
      source_organization_name:
        description: Organization that provided the evaluation
        range: string
      source_organization_url:
        description: URL of the source organization
        range: uri
      evaluator_relationship:
        description: Relationship of evaluator (e.g., first_party, third_party)
        range: string

  ModelInfo:
    is_a: Entity
    class_uri: nexus:modelinfo
    description: Information about the AI model being evaluated
    attributes:
      model_name:
        description: Name of the AI model
        range: string
      model_id:
        description: Identifier of the AI model
        range: string

  SourceData:
    is_a: Entity
    class_uri: nexus:sourcedata
    description: Information about the data source used in evaluation
    attributes:
      dataset_name:
        description: Name of the dataset
        range: string
      source_type:
        description: Type of data source (e.g., hf_dataset)
        range: string
      hf_repo:
        description: HuggingFace repository
        range: string
      hf_split:
        description: HuggingFace dataset split
        range: string

  MetricConfig:
    is_a: Entity
    class_uri: nexus:metricconfig
    description: Configuration for evaluation metrics
    attributes:
      lower_is_better:
        description: Whether lower scores are better
        range: boolean
      score_type:
        description: Type of score (e.g., continuous)
        range: string
      min_score:
        description: Minimum possible score
        range: float
      max_score:
        description: Maximum possible score
        range: float

  ScoreDetails:
    is_a: Entity
    class_uri: nexus:scoredetails
    description: Details about evaluation scores
    attributes:
      score:
        description: The evaluation score
        range: float

  EvaluationResultRecord:
    is_a: Entity
    class_uri: nexus:evaluationresultrecord
    description: A single evaluation result record
    attributes:
      evaluation_name:
        description: Name of the evaluation benchmark
        range: string
    slots:
      - hasSourceData
      - hasMetricConfig
      - hasScoreDetails

  EveryEvalAIResult:
    is_a: AiEvalResult
    class_uri: nexus:everyevalairesult
    description: An evaluation result from the Every Eval Ever dataset, capturing evaluation metadata and results from the EEE_datastore.
    attributes:
      schema_version:
        description: Version of the evaluation schema
        range: string
      evaluation_id:
        description: Unique identifier for this evaluation
        range: string
      evaluation_timestamp:
        description: ISO 8601 timestamp when evaluation was performed
        range: datetime
      retrieved_timestamp:
        description: Unix timestamp when the data was retrieved
        range: string
    slots:
      - hasSourceMetadata
      - hasModelInfo
      - hasEvaluationResults
      - hasDataType
      - hasDomains
      - hasLanguages
      - hasTasks
      - hasDataSource
      - hasDataSize
      - hasDataFormat
      - hasMethods
      - hasMetrics
      - hasLimitations
      - hasGoal
      - hasAudience
      - hasResources
      - hasDocumentation
      - hasRelatedRisk

  BenchmarkMetadataCard:
    is_a: Entity
    class_uri: nexus:benchmarkmetadatacard
    description: "Benchmark metadata cards offer a standardized way to document LLM benchmarks clearly and transparently. Inspired by Model Cards and Datasheets, Benchmark metadata cards help researchers and practitioners understand exactly what benchmarks test, how they relate to real-world risks, and how to interpret their results responsibly.  This is an implementation of the design set out in 'BenchmarkCards: Large Language Model and Risk Reporting' (https://doi.org/10.48550/arXiv.2410.12974)"
    attributes:
      name:
        description: The official name of the benchmark.
      overview:
        description: A brief description of the benchmark's main goals and scope.
    slots:
      - describesAiEval
      - hasDataType
      - hasDomains
      - hasLanguages
      - hasSimilarBenchmarks
      - hasResources
      - hasGoal
      - hasAudience
      - hasTasks
      - hasLimitations
      - hasOutOfScopeUses
      - hasDataSource
      - hasDataSize
      - hasDataFormat
      - hasAnnotation
      - hasMethods
      - hasMetrics
      - hasCalculation
      - hasInterpretation
      - hasBaselineResults
      - hasValidation
      - hasRelatedRisk
      - hasDemographicAnalysis
      - hasConsiderationPrivacyAndAnonymity
      - hasLicense
      - hasConsiderationConsentProcedures
      - hasConsiderationComplianceWithRegulations
      - hasDocumentation

  Question:
    is_a: AiEval
    description: An evaluation where a question has to be answered
    attributes:
      text:
        description: The question itself
        required: true

  Questionnaire:
    is_a: AiEval
    description: A questionnaire groups questions
    slot_usage:
      composed_of:
        range: Question

slots:
  hasImplementation:
    slot_uri: schema:url
    description: A relationship to a implementation defining the risk evaluation
    range: uri
    multivalued: true
    inlined: false
  hasUnitxtCard:
    slot_uri: schema:url
    description: A relationship to a Unitxt card defining the risk evaluation
    range: uri
    multivalued: true
    inlined: false
  hasBenchmarkMetadata:
    description: A relationship to a Benchmark Metadata Card which contains metadata about the benchmark.
    domain: AiEval
    range: BenchmarkMetadataCard
    multivalued: true
    inlined: false
    inverse: describesAiEval
  describesAiEval:
    description: >-
      A relationship where a BenchmarkMetadataCard describes an AI evaluation (benchmark).
    domain: BenchmarkMetadataCard
    range: AiEval
    multivalued: true
    inlined: false
    inverse: hasBenchmarkMetadata
  bestValue:
    description: Annotation of the best possible result of the evaluation
  hasEvaluation:
    slot_uri: dqv:hasQualityMeasurement
    description: A relationship indicating that an entity has an AI evaluation result.
    range: AiEvalResult
    multivalued: true
  isResultOf:
    slot_uri: dqv:isMeasurementOf
    description: A relationship indicating that an entity is the result of an AI evaluation.
    range: AiEval
    multivalued: false
    inlined: false
  hasDataType:
    description: The type of data used in the benchmark (e.g., text, images, or multi-modal)
    multivalued: true
  hasDomains:
    description: The specific domains or areas where the benchmark is applied (e.g., natural language processing,computer vision).
    multivalued: true
  hasLanguages:
    description: The languages included in the dataset used by the benchmark (e.g., English, multilingual).
    multivalued: true
  hasSimilarBenchmarks:
    description: Benchmarks that are closely related in terms of goals or data type.
    multivalued: true
  hasResources:
    description: Links to relevant resources, such as repositories or papers related to the benchmark.
    multivalued: true
  hasGoal:
    description: The specific goal or primary use case the benchmark is designed for.
  hasAudience:
    description: The intended audience, such as researchers, developers, policymakers, etc.
  hasTasks:
    description: The tasks or evaluations the benchmark is intended to assess.
    multivalued: true
    inlined: false
  hasLimitations:
    description: Limitations in evaluating or addressing risks, such as gaps in demographic coverage or specific domains.
    multivalued: true
  hasOutOfScopeUses:
    description: Use cases where the benchmark is not designed to be applied and could give misleading results.
    multivalued: true
  hasDataSource:
    description: The origin or source of the data used in the benchmark (e.g., curated datasets, user submissions).
    multivalued: true
  hasDataSize:
    description: The size of the dataset, including the number of data points or examples.
  hasDataFormat:
    description: The structure and modality of the data (e.g., sentence pairs, question-answer format, tabular data).
    multivalued: true
  hasAnnotation:
    description: The process used to annotate or label the dataset, including who or what performed the annotations (e.g., human annotators, automated processes).
    multivalued: true
  hasMethods:
    description: The evaluation techniques applied within the benchmark.
    multivalued: true
  hasMetrics:
    description: The specific performance metrics used to assess models (e.g., accuracy, F1 score, precision, recall).
    multivalued: true
  hasCalculation:
    description: The way metrics are computed based on model outputs and the benchmark data.
    multivalued: true
  hasInterpretation:
    description: How users should interpret the scores or results from the metrics.
    multivalued: true
  hasBaselineResults:
    description: The results of well-known or widely used models to give context to new performance scores.
    multivalued: true
  hasValidation:
    description: Measures taken to ensure that the benchmark provides valid and reliable evaluations.
    multivalued: true
  hasDemographicAnalysis:
    description: How the benchmark evaluates performance across different demographic groups (e.g., gender, race).
    multivalued: true
  hasConsiderationPrivacyAndAnonymity:
    description: How any personal or sensitive data is handled and whether any anonymization techniques are applied.
    multivalued: true
  hasConsiderationConsentProcedures:
    description: Information on how consent was obtained (if applicable), especially for datasets involving personal data.
    multivalued: true
  hasConsiderationComplianceWithRegulations:
    description: Compliance with relevant legal or ethical regulations (if applicable).
    multivalued: true
  hasSourceMetadata:
    description: Source metadata for the evaluation
    domain: EveryEvalAIResult
    range: SourceMetadata
    inlined: true
  hasModelInfo:
    description: Model information for the evaluation
    domain: EveryEvalAIResult
    range: ModelInfo
    inlined: true
  hasSourceData:
    description: Source data information
    domain: EvaluationResultRecord
    range: SourceData
    inlined: true
  hasMetricConfig:
    description: Metric configuration
    domain: EvaluationResultRecord
    range: MetricConfig
    inlined: true
  hasScoreDetails:
    description: Score details
    domain: EvaluationResultRecord
    range: ScoreDetails
    inlined: true
  hasEvaluationResults:
    description: Array of evaluation results
    domain: EveryEvalAIResult
    range: EvaluationResultRecord
    multivalued: true
    inlined: true
